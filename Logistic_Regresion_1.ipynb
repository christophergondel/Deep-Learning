{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística - Creando una red neuronal simple\n",
    "\n",
    "Este documento implementará el código necesario para construir la arquitectura general de un algoritmo de aprendizaje y finalmente unir las funciones en un modelo principal.\n",
    "Los datos que utilizaremos en esta libreta se han obtenido de la plataforma Coursera y además los puedes encontrar en mi repositorio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El objetivo de este documento es crear un modelo que permita identificar si una imagen es de un gato o no lo es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Vamos a importar todos los paquetes que sean necesarios para trabajar en este notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py #para cargar los sets de datos que vienen en formato H5\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una función load_dataset que cargue el set de entrenamiento y el set e testeo\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # características del set de entrenamiento\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # etiqueta (gato o no gato) \n",
    "\n",
    "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # características del set de testeo\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # etiqueta (gato o no gato)\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) \n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El set de entrenamiento tiene un total de \"m_train\" de imágenes etiquetadas como gato (y=1) o no gato (y=0). Para el caso del set de testeo o prueba tenemos \"m_test\" imágenes. Las dimensiones de las imágenes serán (num_px, num_px, 3), donde num_px es el número de píxeles en altura y anchura, y el 3 es para los canales RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como vamos a pre-procesar estos sets, se añade un _orig al final para diferenciarlos de los que ya\n",
    "#han sido pre-procesados.\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones X entrenamiento: (209, 64, 64, 3)\n",
      "Dimensiones X testeo: (50, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#Veamos las dimensones de las X de entrenamiento y testeo:\n",
    "print('Dimensiones X entrenamiento: ' + str(train_set_x_orig.shape) )\n",
    "print('Dimensiones X testeo: '+ str(test_set_x_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar debemos convertir las matrices en arrays de dimensión (num_px * num_px * 3, 1). Es decir, un vector. Para esto utilizamos reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_vector = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "test_set_x_vector = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a estandarizar nuestros datos dividiendo cada fila del set de datos entre 255, que es el valor máximo de un canal de píxeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_set_x_vector/255\n",
    "x_test = test_set_x_vector/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos diseñar un algoritmo que tenga de entrada x_train, implemente la función con los parámetros w y b, aplique la función sigmoide (que vamos a definir a continuación) y finalmente comience la optimización de la función de coste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b $$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
    "\n",
    "Calculamos la función de coste de la siguiente forma:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la función sigmoide\n",
    "def sigmoid(z):\n",
    "    s = 1/ ( 1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El procedimiento que vamos a seguir es el siguiente:\n",
    "Iniciaremos los parámetros w y b con valor cero. Comenzarán entonces las iteraciones:\n",
    "1) Calcular las pérdidas (propagación hacia delante),\n",
    "2) Calcular el gradiente (propagación hacia atrás) y\n",
    "3) Actualizar parámetros (descenso de gradiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciar los parámetros con ceros\n",
    "def ceros(dims):\n",
    "    \"\"\"\n",
    "    Esta función creará vectores w nulos de dimensión (dims, 1) y hará que b sea cero\n",
    "    dims == tamaño de w\n",
    "    \"\"\"\n",
    "    w = np.zeros((dims,1))\n",
    "    b = 0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora tenemos X y los parámetros W, b. A continuación debemos implementar la función propagación que permita computar la función de coste y su gradiente.\n",
    "- Tenemos X\n",
    "- Calculamos el vector $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Y la función de coste viene dada por  $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las derivadas que necesitaremos son las siguientes:\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagacion(w,b,X,Y):\n",
    "    m= X.shape[1]\n",
    "    \n",
    "    A= sigmoid(np.dot (w.T,X) + b) #Calculamos la activación\n",
    "    cost = -1/m * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A)  ) #Calculamos la función de coste\n",
    "    \n",
    "    dw = 1/m*np.dot(X,(A-Y).T)\n",
    "    db = 1/m*np.sum(A-Y)\n",
    "    \n",
    "    #Utilizamos np.squeeze para eliminar dimensiones en cost\n",
    "    cost = np.squeeze(cost)\n",
    "    return dw , db, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comienza la optimización. Ya tenemos los paráemtros iniciados y hemos definido la función de propagación para calcular la activación, la función de coste y su gradiente. Ahora, debemos ser capaces de actualizar los parámetros a partir del descenso de gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizar(w, b, X, Y, iters, alpha):\n",
    "    \"\"\"\n",
    "    X- datos de entrada\n",
    "    Y- vector de salida con resultados REALES (no probables, sino los de verdad!!)\n",
    "    iters - número de iteraciones\n",
    "    alpha- tasa de aprendizaje (learning rate)\n",
    "\n",
    "\"\"\"\n",
    "    for i in range(iters):\n",
    "        \n",
    "        dw, db, cost = propagacion(w,b,X,Y)\n",
    "    \n",
    "        w = w - alpha*dw\n",
    "        b = b - alpha*db\n",
    "        \n",
    "    parametros = {\"w\": w, \"b\": b }\n",
    "    gradientes = {\"dw\": dw, \"db\": db}\n",
    "    return parametros, gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya tenemos el algoritmo que optimiza los parámetros, queda definir una función que sea capaz de hacer las predicciones. Esto es, calcular $\\hat{Y}$ y además convertir los resultados en 0 o en 1 según la activación sea mayor o menor que 0.5; para guardar las predicciones crearemos un vector llamado \"Y_preds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir(w,b,X):\n",
    "    m= X.shape[1]\n",
    "    Y_preds = np.zeros((1,m)) #Creamos el vector de predicciones con ceros\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A= sigmoid(np.dot(w.T,X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0][i] >0.5:\n",
    "            Y_preds[0][i]=1\n",
    "        else:\n",
    "            Y_preds[0][i]=0\n",
    "    return Y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De momento...\n",
    "\n",
    "Hemos iniciado los parámetros w y b. Luego, optimizamos de forma iterativa para obtener los parámetros w y b. Por último se realizaron predicciones utilizando los parámetros aprendidos.\n",
    "\n",
    "Ahora queda implementarlo todo en un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_final(X_train, Y_train, X_test, Y_test, iterations, alpha):\n",
    "    w, b = ceros(X_train.shape[0]) #Iniciar los parámetros en cero\n",
    "    parametros,gradientes = optimizar(w,b, X_train, Y_train, iterations, alpha) #Optimizar\n",
    "    w = parametros['w'] #Cogemos el w aprendido\n",
    "    b = parametros['b'] #Cogemos el b aprendido\n",
    "    \n",
    "    #Vamos a hacer las predicciones en los sets de entrenamiento y testeo\n",
    "    Y_pred_train = predecir(w,b,X_train)\n",
    "    Y_pred_test = predecir(w,b,X_test)\n",
    "    \n",
    "    print(\"Precisión entrenamiento: {} %\".format(100 - np.mean(np.abs(Y_pred_train - Y_train)) * 100))\n",
    "    print(\"Precisión test: {} %\".format(100 - np.mean(np.abs(Y_pred_test - Y_test)) * 100))\n",
    "    \n",
    "    final = {\"w\": w, \"b\": b, \"Predicción Y entrenamiento\":Y_pred_train, \"Predicción Y testeo\":Y_pred_test}\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión entrenamiento: 98.56459330143541 %\n",
      "Precisión test: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "final = modelo_final(x_train, train_set_y, x_test, test_set_y, iterations= 1800, alpha= 0.005 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, nuestro modelo tiene una precisión con el set de entrenamiento de un 98%. Para el caso del set de testeo tenemos un 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
